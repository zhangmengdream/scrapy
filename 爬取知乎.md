#### session和cookie的区别

cookie是浏览器支持的一种本地存储方式，相当于dict，键值对的形式

其实他在浏览器存储的是一段文本，浏览器会自动对文本进行解析

http本身是无状态的

- 用户第一次请求之后页面进行，服务器直接返回
- 用户第二次发起请求，与第一次之间是没有任何联系的



![](https://github.com/zhangmengdream/scrapy/blob/master/%E6%97%A0%E6%A0%87%E9%A2%98.png?raw=true)



比如淘宝，在未登录的情况下，淘宝也是记住的某个用户A浏览量哪些商品，B浏览了哪些商品

如果只是用http本身的一种无状态的协议是没有办法做到的

所以需要有状态的请求

浏览器A和浏览器B能够分别标记，就是使用cookie

浏览器A向服务器发起请求的时候，服务器自动给浏览器A生成一个id=1返回给浏览器，

浏览器A把id放到本地cookie中（浏览器因为安全性，cookie存储是会有很多要求的，比如服务器A返回的cookie是放在服务器A的域之下的，（比如百度的cookie就不能够和jobbole的cookie互相访问）是不能够跨域访问的，这是一种安全机制），

在下一次请求的时候，浏览器会带着cookie中的id=1向服务器做请求，服务器就可以知道是哪个客户端了

![52749229282](C:\Users\dream\AppData\Local\Temp\1527492292822.png)

cookie的机制就是每次请求的时候吧cookie给我们的信息存储到本地，每次访问的时候把这些信息返回给服务器，这样就可以完成一些用户信息的保存



为了不每次访问网站的时候都需要登录，所以我们采用的模式就是，每次向服务器发起请求的时候，都会带上cookie中的用户名和密码，

登录的时候把用户名和密码都保存到cookie中来，然后把这些发送给后台服务器，在后台服务器对

这些用户名和密码做验证，然后进行登录，这样也是完成cookie的登录

这样就会有了安全的问题，所以用session

把sessionid  每次请求的时候都带过来，浏览器就不需要知道你的密码，就可以直接了，sessionid有过期时间

session是服务器生成的，是存储在服务器端的



#### 常见的httpcode

| code    | 说明                   |
| ------- | ---------------------- |
| 200     | ok                     |
| 301/302 | 永久跳转、临时跳转     |
| 403     | 没有权限访问           |
| 404     | 没有对象的资源         |
| 500     | 服务器错误             |
| 503     | 服务器停机或者正在维护 |

#### requests库

```python





```



#### requests方式 模拟登陆

```python
首先分析网站：
分析post之后是发送什么数据过去的，post到哪个url
可以先通过输入错误的账号密码，获取登录请求的过程

# 通过resuest完成知乎的模拟登录

# cookielib  通过读本地的cookie文件
# 将他生成cookie并把他赋给requests的cookie

try：
	import cookielib
except:
    import http.cookiejar as cookielib
import requests
import re
# 用requests实例化一个session变量，有了这个session之后，就可以不用requests库来请求了，用session就可以了。
# session代表某一个次链接，这样就不需要每次requests.get每次都建立一次连接了
# 这样效率更高（用session比requests获取页面效率高）

session = requests.session()

# LWPCookieJar这个类实例化出来的cookie，可以直接调用save方法，可以让我们在保存文件读取文件的时候变得很容易（指明文件名，没有的话会自动创建）
session.cookies = cookielib.LWPCookieJar(filename="cookies.txt")

# 登录一次后cookie已经保存成功的话，这里就可以把cookie  load进来了
# load了cookie之后，就不再会为我们返回首页了

try:
    session.cookies.load(ignore_discard=True)
except:
    print("cookie未能加载")

# 如果已经把cookie保存到了本地，就不需要调用login函数了，可以直接运行这个请求知乎的页面
# 会直接返回内容页面 （注意这里有时候可能会出现编码的错误，需要encode成utf-8）
def get_index():
    response = session.get("https://www.zhihu.com",headers=header)
    with open("index_page.html","wb") as f:
        f.write(response.text.encode("utf-8"))
    print("ok")
    
# 用requests的get方法的时候，requests给我们设置默认header的时候，默认设置的不是浏览器的header。即User-Agent代理
# 每个浏览器对应不同的user-agent,有的浏览器会验证你的user-agent是否合法,如果不在他的列表里面就会屏蔽，或者发出一个500的错误
# 这个headers在获取某一个页面的时候传递进去

agent = "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/66.0.3359.181 Safari/537.36"
headers = {
    "HOST":"https://www.zhihu.com/signup",
    "referer":"https://www.zhihu.com/signup"
    "user-agent":agent
}


def get_xsrf():
    response = session.get("https://www.zhihu.com/signup",headers=headers)
    #这里就可以获取值了，就可以做正则表达式的匹配了
    xstf = re.match(".*name="_xsrf" value="(.*?)",response.text)
    
def zhihu_login(username,password):

    #在这里配置发起post请求登录所需要的参数               
	post_url = "https://www.zhihu.com/signin"
	post_data = {
         "_xsrf":get_xsrf(),
         "phone_num":username,
         "password":password
     }
   # 模拟登陆 （就是将这些数据post到某一个url）这时服务器就会返回一个response
   # 这个response里面就会带一些cookie值，会从服务器得到数据
     response_text = session.post(post_url,data=post_data,headers=headers)
   # 这里用session post之后 就是rest接口的一个返回      
   # 这里可以直接将session返回的cookie直接做save,保存到本地
                    
   # 有了上面的LWPCookieJar的方法打开了一个文件，这里就可以直接保存cookie了
     session.cookies.save()
                    
#调用函数，输入用户名和密码          
zhihu_login(username,password)

#检测用户是否是登陆状态，用一个登陆后才可以进入的url进行检测，如果检测302跳转到登陆页面说明未登录

# session做请求的时候，如果服务器给我们发回来的是302，他会自动获取302之后的页面，即登陆页面,这样之后就会返回200的状态码，我们无法判断，所以需要设置一个参数
# allow_redirects=False 不允许重定向跳转的意思
                    
def is_login():
    inbox_url = 'https://www.zhihu.com/inbox'
    response = session.get(inbox_url,headers=header,allow_redirects=False)	
    if response.status_code != 200:
        return False
    else :
        return True         

                    
```

----------------------------------------------------------------------------------------------------

#### scrapy 方式 模拟登陆

```python


# 通过scrapy完成知乎的模拟登录

scrapy genspider zhihu www.zhihu.com



```

#### urllib

```python
spider页面
spider 的入口是放在start_requests里面的
class zhihuspider(spider):
    

```









```python
#登录网址
https://accounts.douban.com/login?alias=13426039389&redir=https%3A%2F%2Fwww.douban.com&source=group&error=1012
    
豆瓣 豆瓣的post请求的邮箱和手机号的网址都一样（post到同一个url）
邮箱：手机号：
post
accounts.douban.com(127.0.0.1:55859)

参数：
form_email	27378892514@qq.com
form_password	zhangmengmeng

#name	value

login	登录 
redir	https://www.douban.com
source	group


取到以上参数，拿这些参数对服务器做一个post，做了post之后，服务器会给我们返回一个cookie，会告诉我们下一次登录的时候，会把哪些值带回来，带回来之后，我们就可以识别是哪一个用户。所以登录之后一定要把cookie保留下来，下一次请求页面的时候，就把cookie带回去

```

captcha-id

captcha-solution



```javascript
captcha-id  eVwJE0Mxss19J3mzT6SyB34j:en
captcha-solution error
form_email 13426039389
form_password zhangmengjie
login 登录
redir https://accounts.douban.com/
source None



captcha-id	Aakyg2VPwHLkWnrHYoEh4A6c:en
captcha-solution	sheep
form_email	13426039389
form_password	aaa
login	登录
redir	https://accounts.douban.com/
source	None
```

```python
使用pytesseract识别验证码







```





```python
模拟登陆还需要判断用户是否已经登录
用一个登陆之后能访问的页面来检测，如果返回的状态码为302（重定向到登录页面），则需要登陆
如果返回的页面是200 ，则是已经登录

```







https://www.douban.com/mine/wallet/





正则表达式默认只匹配一行

加参数 re.DOTALL  就会按照整个数据来匹配









https://www.zhihu.com/lives/827530183664861184/related









下拉刷新的如何爬取，怎样找规则

[https://api.zhihu.com/lives/827530183664861184/related?limit=5&offset=15]()

接口





https://www.zhihu.com/lives/901413690908241920

```python
def parse_answer(self,response):
        ans_json = json.loads(response.text)
        print(ans_json)
获取json数据
```

https://www.douban.com/misc/captcha?id=cZEtuAdeCsxoNXqlTd10bwic:en&size=s

https://www.douban.com/misc/captcha?id=jF2agnKrgdMF8x9Un7EIoenH:en&size=s

python3  ------  input

python2  ------  raw_input





id=        jF2agnKrgdMF8x9Un7EIoenH           :enz







yield 一个请求图片的request

保证在同一个cookie之下





callback很重要

在login中yield Request之后，在 login_after_captcha  里面请求url之后，和code是在同一个cookie

之下请求的



将图片验证码的过程放在了 login_after_captcha() 中了，  请过request请求的图片

如果请求图片直接在login中做， 不通过scrapy.Request来做的话，，登录会出错 

从response中拿到cookie，，然后把这个cookie设置到request中去

这样就可以保证两边的request是同一个session

参数通过meta 传递到response中去

meta={"post_data":'post_data'}



post_data = response.metaget(post_data)

得到图片yield给登陆之后，可以保证在同一个cookie之下







登录之后，再次请求会带去cookie和session 



异步的框架，所以必须yield出去  return出去的request没有用



相当于login往后延迟了一步，把验证码获取之后，在登录（保证在同一个COOKIE之下请求）





/html/body/div[2]/div/div[1]/div/span



```python
jiodshfiuhi
```































