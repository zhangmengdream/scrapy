#### session和cookie的区别

cookie是浏览器支持的一种本地存储方式，相当于dict，键值对的形式

其实他在浏览器存储的是一段文本，浏览器会自动对文本进行解析

http本身是无状态的

- 用户第一次请求之后页面进行，服务器直接返回
- 用户第二次发起请求，与第一次之间是没有任何联系的



![](https://github.com/zhangmengdream/scrapy/blob/master/%E6%97%A0%E6%A0%87%E9%A2%98.png?raw=true)



比如淘宝，在未登录的情况下，淘宝也是记住的某个用户A浏览量哪些商品，B浏览了哪些商品

如果只是用http本身的一种无状态的协议是没有办法做到的

所以需要有状态的请求

浏览器A和浏览器B能够分别标记，就是使用cookie

浏览器A向服务器发起请求的时候，服务器自动给浏览器A生成一个id=1返回给浏览器，

浏览器A把id放到本地cookie中（浏览器因为安全性，cookie存储是会有很多要求的，比如服务器A返回的cookie是放在服务器A的域之下的，（比如百度的cookie就不能够和jobbole的cookie互相访问）是不能够跨域访问的，这是一种安全机制），

在下一次请求的时候，浏览器会带着cookie中的id=1向服务器做请求，服务器就可以知道是哪个客户端了

![52749229282](https://github.com/zhangmengdream/scrapy/blob/master/%E5%BE%AE%E4%BF%A1%E5%9B%BE%E7%89%87_20180528152443.png?raw=true)

cookie的机制就是每次请求的时候吧cookie给我们的信息存储到本地，每次访问的时候把这些信息返回给服务器，这样就可以完成一些用户信息的保存



为了不每次访问网站的时候都需要登录，所以我们采用的模式就是，每次向服务器发起请求的时候，都会带上cookie中的用户名和密码，

登录的时候把用户名和密码都保存到cookie中来，然后把这些发送给后台服务器，在后台服务器对

这些用户名和密码做验证，然后进行登录，这样也是完成cookie的登录

这样就会有了安全的问题，所以用session

把sessionid  每次请求的时候都带过来，浏览器就不需要知道你的密码，就可以直接了，sessionid有过期时间

session是服务器生成的，是存储在服务器端的



#### 常见的httpcode

| code    | 说明                   |
| ------- | ---------------------- |
| 200     | ok                     |
| 301/302 | 永久跳转、临时跳转     |
| 403     | 没有权限访问           |
| 404     | 没有对象的资源         |
| 500     | 服务器错误             |
| 503     | 服务器停机或者正在维护 |

#### requests库

```python


登陆完之后，一定要把cookie给我们保存下来，然后，下次请求的时候要把我们的cookie带过去



```



#### requests方式 模拟登陆

```python
首先分析网站：
分析post之后是发送什么数据过去的，post到哪个url
可以先通过输入错误的账号密码，获取登录请求的过程

# 通过resuest完成知乎的模拟登录
# cookielib是python自带的
# cookielib  通过读本地的cookie文件
# 将他生成cookie并把他赋给requests的cookie

try：
	import cookielib
except:
    import http.cookiejar as cookielib
import requests
import re
# 用requests实例化一个session变量，有了这个session之后，就可以不用requests库来请求了，用session就可以了。
# session代表某一个次链接，这样就不需要每次requests.get每次都建立一次连接了
# 这样效率更高（用session比requests获取页面效率高）

requests.session 这个session代表的是某一次连接
session = requests.session()

# LWPCookieJar这个类实例化出来的cookie，可以直接调用save方法，可以让我们在保存文件读取文件的时候变得很容易（指明文件名，没有的话会自动创建）
session.cookies = cookielib.LWPCookieJar(filename="cookies.txt")

# 登录一次后cookie已经保存成功的话，这里就可以把cookie  load进来了
# load了cookie之后，就不再会为我们返回首页了

try:
    session.cookies.load(ignore_discard=True)
    #这里sessionload之后，再用session请求的时候，就会把cookie带过去
except:
    print("cookie未能加载")

# 如果已经把cookie保存到了本地，就不需要调用login函数了，可以直接运行这个请求知乎的页面
# 会直接返回内容页面 （注意这里有时候可能会出现编码的错误，需要encode成utf-8）
def get_index():
    response = session.get("https://www.zhihu.com",headers=header)
    with open("index_page.html","wb") as f:
        f.write(response.text.encode("utf-8"))
    print("ok")
    
# 用requests的get方法的时候，requests给我们设置默认header的时候，默认设置的不是浏览器的header。即User-Agent代理
# 每个浏览器对应不同的user-agent,有的浏览器会验证你的user-agent是否合法,如果不在他的列表里面就会屏蔽，或者发出一个500的错误
# 这个headers在获取某一个页面的时候传递进去

agent = "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/66.0.3359.181 Safari/537.36"
headers = {
    "HOST":"https://www.zhihu.com/signup",
    "referer":"https://www.zhihu.com/signup"
    "user-agent":agent
}


def get_xsrf():
    response = session.get("https://www.zhihu.com/signin",headers=headers)
    #这里就可以获取值了，就可以做正则表达式的匹配了
    xstf = re.match(".*name="_xsrf" value="(.*?)",response.text)
    if xstf:
        return (xsrf.group(1))
    else:
        return ''
def zhihu_login(username,password):

    #在这里配置发起post请求登录所需要的参数               
	post_url = "https://www.zhihu.com/signin"
	post_data = {
         "_xsrf":get_xsrf(),
         "phone_num":username,
         "password":password
     }
   # 模拟登陆 （就是将这些数据post到某一个url）这时服务器就会返回一个response
   # 这个response里面就会带一些cookie值，会从服务器得到数据
     response_text = session.post(post_url,data=post_data,headers=headers)
   # 这里用session post之后 就是rest接口的一个返回      
   # 这里可以直接将session返回的cookie直接做save,保存到本地
                    
   # 有了上面的LWPCookieJar的方法打开了一个文件，这里就可以直接保存cookie了
     session.cookies.save()
                    
#调用函数，输入用户名和密码          
zhihu_login(username,password)

#检测用户是否是登陆状态，用一个登陆后才可以进入的url进行检测，如果检测302跳转到登陆页面说明未登录

# session做请求的时候，如果服务器给我们发回来的是302，他会自动获取302之后的页面，即登陆页面,这样之后就会返回200的状态码，我们无法判断，所以需要设置一个参数
# allow_redirects=False 不允许重定向跳转的意思
                    
def is_login():
    inbox_url = 'https://www.zhihu.com/inbox'
    response = session.get(inbox_url,headers=header,allow_redirects=False)	
    if response.status_code != 200:
        return False
    else :
        return True         

                    
```

----------------------------------------------------------------------------------------------------

#### scrapy 方式 模拟登陆

```python
spider页面:
spider 的入口是放在start_requests里面的,这个是爬虫的入口，所以我们需要在爬虫的入口这里完成模拟登陆，所以需要重载这个函数
import re
import json
import scrapy


class zhihuspider(scrapy.spider):
	name='zhihu'
    allowed_domains-['www.zhihu.com']
    start_urls=['http://www.zhihu.com']
    agent = "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/66.0.3359.181 Safari/537.36"
    
    headers = {
    "HOST":"https://www.zhihu.com/signup",
    "referer":"https://www.zhihu.com/signup"
    "user-agent":agent
	}
    
    def parse(self,response)
		pass
    
    def start_requests(self):
        #获取首页html信息
        return [scrapy.Request('https://www.zhihu.com/signin'，callback=self.login)]
    
    def login(self,response)
    # 有了response之后，就可以在这里提取code
        text = response.text
        xstf_obj = re.match(".*name="_xsrf" value="(.*?)",response.text)
        xstf = ''             
        if xstf_obj:
            xstf = xstf_obj.group(1))
        if xsrf:
            postdata={
             "_xsrf":xstf,
             "phone_num":1878905378,
             "password":password
            }
             return [scrapy.FromRequest(
                url="https://www.zhihu.com/signin",
                 formdata = postdata
                 headers=self.headers
                 callback=self.check_login
             )]
    # 验证服务器的返回数据，判断是否登陆成功
    def check_login(self,response)
    #通过response分析可以看出，登陆成功的时候response.text 是返回登陆成功的字样说明登陆成功了，所以这里可以检测一下response
    	text_json = json.loads(response.text)
        # 在这里判断如果登陆成功了就开始爬取数据
    	if 'msg' in text_json and text_json['msg']='登陆成功'：
        	for url in self.start_urls:
                yield scrapy.Request(url,dont_filter=True,headers=self.headers)
                
                # yield self.make_requests_from_url(url)源码中这里生成的request并没有给添加headers，所以我们需要自己添加这个逻辑
        	    # 这里不写回调函数，会默认走到parse里面去的，debug查看parse里面response即可看到结果，然后就可以执行一系列爬取数据的操作了
        
# FromRequest是在scrapy里面的，可以直接点进来。
# FromRequest可以完成我们的表单提交,可以在这里面完成requests中的登录的操作
# FromRequest有两个参数
# 1.url
# 2.formdata 和post的data是一样的 是指提交的数据
# 

#scrapy是一个异步的框架，他的请求是异步的，所以需要设置回调函数，callback，如果不设置callback的话会默认调用parse函数

# 在scrapy中，不用把cookie专门保存起来，因为在后面所有跟踪的request中
# 他都会默认把cookie放置进去，所以我们是直接可以请求下一个url的scrapy.
```

#### scrapy shell 里面也是可以写入useragent的

```python
scrapy shell -s useragent  链接

点击更多，页面加长，这样的情况，分析

在做爬虫的时候，分析一个网站url请求的结构是非常重要的，他会让我们更加清除我们需要发什么请求，以及这个请求的格式是什么
```

分析：

```python
爬取评论的时候，当评论数很多的时候，需要点击查看更多才能够请求到下面的内容的时候，需要分析页面，点击查看更多的时候，会有下面的网址，返回json数据，按照offset的值，可以得到，每页数据请求的网址，来判断爬取数据

https://api.zhihu.com/lives/827530183664861184/related?limit=5&offset=5
```

### 爬取知乎具体字段

```python
scrapy默认采用深度优先的算法

def parse(self,response):
    '''
    提取出html页面中的所有url，并跟踪这些url进行一步爬取
    如果提取的url格式为question/xxx 就上下载后直接进入解析函数
    '''
    all_urls = response.css("a::attr(href)").extract()
    all_urls = [parse.urljoin(response.url,)]
```





captcha-id	qzyv2GBP1YWNWBQiPdEM2uek:en
captcha-solution	thougth
form_email	2737892514@qq.com
form_password	aaaaa
login	登录
redir	https://www.douban.com/
source	index_nav





![52841777701](C:\Users\dream\AppData\Local\Temp\1528417777016.png)

unicode和utf-8编码的意思





```python
#登录网址
https://accounts.douban.com/login?alias=13426039389&redir=https%3A%2F%2Fwww.douban.com&source=group&error=1012
    
豆瓣 豆瓣的post请求的邮箱和手机号的网址都一样（post到同一个url）
邮箱：手机号：
post
accounts.douban.com(127.0.0.1:55859)

参数：
form_email	27378892514@qq.com
form_password	zhangmengmeng

#name	value

login	登录 
redir	https://www.douban.com
source	group


取到以上参数，拿这些参数对服务器做一个post，做了post之后，服务器会给我们返回一个cookie，会告诉我们下一次登录的时候，会把哪些值带回来，带回来之后，我们就可以识别是哪一个用户。所以登录之后一定要把cookie保留下来，下一次请求页面的时候，就把cookie带回去

```

captcha-id

captcha-solution



```javascript
captcha-id  eVwJE0Mxss19J3mzT6SyB34j:en
captcha-solution error
form_email 13426039389
form_password zhangmengjie
login 登录
redir https://accounts.douban.com/
source None



captcha-id	Aakyg2VPwHLkWnrHYoEh4A6c:en
captcha-solution	sheep
form_email	13426039389
form_password	aaa
login	登录
redir	https://accounts.douban.com/
source	None
```

```python
使用pytesseract识别验证码







```





```python
模拟登陆还需要判断用户是否已经登录
用一个登陆之后能访问的页面来检测，如果返回的状态码为302（重定向到登录页面），则需要登陆
如果返回的页面是200 ，则是已经登录

```







https://www.douban.com/mine/wallet/





正则表达式默认只匹配一行

加参数 re.DOTALL  就会按照整个数据来匹配









https://www.zhihu.com/lives/827530183664861184/related









下拉刷新的如何爬取，怎样找规则

[https://api.zhihu.com/lives/827530183664861184/related?limit=5&offset=15]()

接口





https://www.zhihu.com/lives/901413690908241920

```python
def parse_answer(self,response):
        ans_json = json.loads(response.text)
        print(ans_json)
获取json数据
```

https://www.douban.com/misc/captcha?id=cZEtuAdeCsxoNXqlTd10bwic:en&size=s

https://www.douban.com/misc/captcha?id=jF2agnKrgdMF8x9Un7EIoenH:en&size=s

python3  ------  input

python2  ------  raw_input





id=        jF2agnKrgdMF8x9Un7EIoenH           :enz







yield 一个请求图片的request

保证在同一个cookie之下





callback很重要

在login中yield Request之后，在 login_after_captcha  里面请求url之后，和code是在同一个cookie

之下请求的



将图片验证码的过程放在了 login_after_captcha() 中了，  请过request请求的图片

如果请求图片直接在login中做， 不通过scrapy.Request来做的话，，登录会出错 

从response中拿到cookie，，然后把这个cookie设置到request中去

这样就可以保证两边的request是同一个session

参数通过meta 传递到response中去

meta={"post_data":'post_data'}



post_data = response.metaget(post_data)

得到图片yield给登陆之后，可以保证在同一个cookie之下







登录之后，再次请求会带去cookie和session 



异步的框架，所以必须yield出去  return出去的request没有用



相当于login往后延迟了一步，把验证码获取之后，在登录（保证在同一个COOKIE之下请求）





/html/body/div[2]/div/div[1]/div/span



```python
jiodshfiuhi
```













18.06 保存副本

```python
class JSPageMiddleware(object):

    def getimg(self,browser,request):
        try:
            captcha_img = re.match('.*?<img id="captcha_image" src="(.*?)" alt="captcha" class="captcha_image">.*?',browser.page_source, re.DOTALL)
            captcha_img = captcha_img.group(1)
            from PIL import Image
            browser.save_screenshot("code.jpg")
            img = Image.open("code.jpg")
            region = (10, 10, 100, 100)
            nimg = img.crop(region)
            nimg.save("new_code.jpg")

            # t = browser.get(captcha_img)
            # time.sleep(3)
            # with open("new_code.jpg", "wb") as f:
            #     f.write(nimg.content)
            #     f.close()
            try:
                im = Image.open('new_code.jpg')
                im.show()
                im.close()
                browser.find_element_by_css_selector("")
            except:
                browser.find_element_by_css_selector(".btn-submit").click()
        except:
            browser.find_element_by_css_selector(".btn-submit").click()
            try:
                captcha_img = re.match('.*?<img id="captcha_image" src="(.*?)" alt="captcha" class="captcha_image">.*?',browser.page_source, re.DOTALL)
                captcha = captcha_img.group(1)
                jpg_link = captcha  # 图片链接
                path = os.path.abspath(__file__)
                from urllib import request
                request.urlretrieve(jpg_link, path+'/captch.jpg')

                # request.urlretrieve(jpg_link,path)  # path为路径加名字哦（如 ~/workjpg/111.jpg）！！！如果不需要路径，也要有个名字，如 111.jpg就直接保存在当前目录下
                
                # browser.save_screenshot("code.jpg")
                # img = Image.open("code.jpg")
                # region = (10, 10, 100, 100)
                # nimg = img.crop(region)
                # nimg.save("new_code.jpg")

                # t = browser.get(captcha_img)
                # time.sleep(3)
                # with open("new_code.jpg", "wb") as f:
                #     f.write(nimg.content)
                #     f.close()
                try:
                    from PIL import Image
                    im = Image.open('captch.jpg')
                    im.show()
                    im.close()
                    browser.find_element_by_css_selector("")
                except:
                    browser.find_element_by_css_selector(".btn-submit").click()
            except:
                return HtmlResponse(url=browser.current_url, body=browser.page_source, encoding="utf-8",request=request)

    def process_request(self, request, spider):
        if spider.name=='dbspider':
            browser = webdriver.Firefox(executable_path='/home/test/Desktop/geckodriver')
            browser.get(request.url)

            time.sleep(5)

            browser.find_element_by_css_selector("input[name='form_email']").send_keys('2737892514@qq.com')
            time.sleep(2)
            browser.find_element_by_css_selector("input[id='password']").send_keys('zhangmengjie')
            time.sleep(5)

            try:
                self.getimg(browser,request)
            except:
                self.getimg(browser,request)

            browser.find_element_by_css_selector(".btn-submit").click()



            return HtmlResponse(url=browser.current_url, body=browser.page_source, encoding="utf-8",request=request)
```







```python
import requests
from scrapy.selector import Selector
import MySQLdb

conn = MySQLdb.connect(host='127.0.0.1',user='root',passwd = 'test1999',db = 'douban' ,charset='utf8' )
cursor = conn.cursor()


def xici_ips():
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/67.0.3396.79 Safari/537.36',
    }
    for i in range(1000):
        re = requests.get("http://www.xicidaili.com/nn/{0}".format(i),headers=headers)
        selector = Selector(text=re.text)
        all_trs = selector.css('#ip_list tr')

        ip_list = []
        for tr in all_trs[1:]:
            speed_str = tr.css(".bar::attr(title)").extract()[0]
            if speed_str:
                speed = float(speed_str.split('秒')[0])
            all_texts=tr.css("td::text").extract()

            ip = all_texts[0]
            port = all_texts[1]
            proxy_type = all_texts[5]

            ip_list.append(((ip,port,proxy_type,speed)))

        for ip_info in ip_list:
            cursor.execute(
                "insert into proxy_ip(ip,port,speed,proxy_type) VALUES('{0}','{1}',{2},'HTTP')".format(ip_info[0],ip_info[1],ip_info[3]))

            conn.commit()
class GetIp(object):
    def delete_ip(self,ip):
        delete_sql = '''
            delete from proxy_ip WHERE ip='{0}'
        '''.format(ip)
        cursor.execute(delete_sql)
        conn.commit()
        return True

    def judge_ip(self,ip,port):
        http_url = 'http://www.baidu.com'
        proxy_url = 'http://{0}:{1}'.format(ip,port)

        try:
            proxy_dict = {
                "http":proxy_url
            }
            response = requests.get(http_url, proxies=proxy_dict)

        except Exception as e:
            print('invalid ip and port')
            self.delete_ip(ip)
            return False
        else:
            code = response.status_code
            if code>=200 and code<300:
                print('effective ip')
                return True
            else:
                print('invalid ip and port')
                self.delete_ip(ip)
                return False

    def get_random_ip(self):
    #从数据库中随机取出一个可用的ｉｐ
        random_sql = '''
        select ip,port from proxy_ip ORDER BY RAND() limit 1
        '''
        cursor.execute(random_sql)
        for ip_info in cursor.fetchall():
            ip = ip_info[0]
            port = ip_info[1]

            judge_re = self.judge_ip(ip,port)
            if judge_re:
                return "http://{0}:{1}".format(ip,port)
            else:
                return self.get_random_ip()

# xici_ips()
if __name__ == '__main__':
    get_ip = GetIp()
    get_ip.get_random_ip()

















```











