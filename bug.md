##  request和response

```markdown
# request：

cookie是可以自动设置的，不过自动登录之后，scrapy就会自动将cookie加入到request当中.

scrapy会有一个默认的middleware
# 参数：
1. cookie：  可以使一个dict，也可以是一个list
2. priority: 这个参数是用来影响scheduler的优先调度（比如一个request后来，下一次调度的时候，如果设置的priority比较高，会优先调度这个request）
3. dont_filter: 表名这个request不应该被过滤（当设置为True的时候，这个request是会被过滤掉的） --  当你在同一时刻，发送多个request的时候，希望不要被过滤掉，就可以设置他为false
4. errback 如果在处理请求时出现异常，将调用该函数,做一下后续的处理。这包括了404错误的页面。



```

```markdown
# response
返回的参数
url
status
headers
body
flags
request --- 之前yield出去的request  可以知道当时是对哪个request做的解释

```



## 如何随机的切换user-Agent

```markdown
user-Agent 用户代理
可以让服务器能够识别用户使用的操作系统，以及版本，CPU类型，浏览器版本，浏览器的渲染引擎，浏览器语言等等

# 如何随机的切换user-Agent



```



## DOWNLOADER_MIDDLEWARES

```markdown
和pipeline一样，需要自己配置class  后面的数字代表执行的顺序





```





### fake-useragent

基于github 开源的专门用来随机切换user-agent的

第一步 安装：

pip install fake-useragent

用法：

```python
>>> from fake_useragent import UserAgent
>>> a = UserAgent()

# 他会随机切换不同的版本
>>> a.ie
'Mozilla/5.0 (compatible; MSIE 10.0; Windows NT 6.1; Trident/5.0)'
>>> a.firefox
'Mozilla/5.0 (Windows NT 6.1; WOW64; rv:21.0) Gecko/20130401 Firefox/21.0'


# 直接random，会在不同的浏览器之间随机的切换，切换系统
>>> a.random
'Mozilla/5.0 (compatible; MSIE 8.0; Windows NT 6.0; Trident/4.0; InfoPath.1; SV1; .NET CLR 3.8.36217; WOW64; en-US)'

```

#### 代码块：

```python
from fake_useragent import UserAgent
class RandomUserAgentMiddlware(object):
    # 随机更换user-agent

    def __init__(self,crawler):
        super(RandomUserAgentMiddlware,self).__init__()
        self.ua = UserAgent()
    @classmethod
    def from_crawler(cls,crawler):
        return cls(crawler)

    def process_request(self,request,spider):
        request.headers.setdefault('User_Agent',self.ua.random)




```







### ip的变化方案，以及如何设置ip代理



西刺免费代理IP里面提供ip，自己写一个爬虫爬取里面的ip，爬取后放入数据库，或者文件当中，就有了ip代理的数据源



HTTP 和 HTTPS 是ip代理的一种类型

```python
将ip爬取出来，存入数据库， ，
写一个类，在从数据库动态获取ip，，检测ip是否可用，用百度网站请求检测，将不可用的ip删除。  
引用类，在middleware里来设置ip动态代理

代码：
import requests
from scrapy import selector 

class ip_dali(response):
    
    def 




```



# scrapy-proxies

```python
proxies   代理







```







限速很重要 最稳定的ip还是自己的ip



tor洋葱网络，可以层层包装，完成匿名访问，黑客用的比较多，比较安全，比网上搜索的ip代理，甚至收费的ip代理都要稳定一些









怎么限速？？？





收费的Ip代理

pip install crawlera



```pu
通过selenium来模拟登陆知乎的时候，需要先设置点击事件，才可以真正进入登陆页面
```

模拟登陆成功后，就可以用browser来爬取其他页面的内容，怎么做？



### 已解决：

##### 当一个字段有多个值得时候，怎么存储  ------  拼接成字符串

scrapy的url去重原理   ？？？？？



### 问题：



当请求频率过快的时候 中间需要填写验证码，怎么做 ？？？？？？？？？

怎么爬app  --------    如何用Fiddler对Android应用进行抓包 ？？？？？？？？

怎么爬视频 ？？？？？？？？？ 

多进程多线程爬虫 ？？？？？



缓存？？？？？？

如何将selenium集成到scrapy中   写一个中间件  返回httpresponse对象

如何将scrapy_redis集成到scrapy中

如何将 bloomfilter  集成到scrapy-redis中，改源码，在源码中添加 。。。。。

。。。。。。。。。。

如何将elasticasearch集成到scrapy中



selenium获取的数据为什么打印不出html？？？







今日头条，百度等，怎么用的爬虫？   elasticsearch

爬虫算是开发吗

![](C:\Users\dream\Desktop\QQ图片20180527112742.png)







(1)采用scrapy-redis分布式实现。

(2)攻破403反爬技术，所以必须通过Fiddler抓取二次访问的请求报头



## 2017.08 - 2017.09  华强芯城项目爬取

| 软件环境： | linux                                                        |
| ---------- | ------------------------------------------------------------ |
| 开发工具： | pycharm                                                      |
| 责任描述： | 1.需求分析、技术方案选型，项目的架构设计、开发环境搭建；  2.爬虫模块：对目标站点页面解析，进行深度抓取；  3.中间件：构造请求和响应，处理特殊页面。  4.管道模块：对关键目标信息进行持久化存储。  5.爬取策略制定，友好抓取数据，防止反爬。 |
| 项目描述： | 此项目是对华强芯城相关上品信息的爬取，采用scrapy框架用xpath节点爬取的数据分析电子元器件信息，快速捕捉市场机遇，方便公司即时掌握本行业行情信息，做出战略调整实现利益最大化，同时将   爬取的资讯进行筛选，存储带公司的数据库中。 |

















 2016.08 - 2016.08  腾讯招聘网

| 项目描述： | 腾讯招聘网  开发环境：Ubuntu、scrapy框架、scrapy-redis分布式组件、Redis  项目简介：这个项目是对找工作方面信息的抓取，采用scrapy-redis分布式  实现。分布式使用Redis做为缓存数据库，利用Redis的高并发和I/O读写来  实现高速下载。抓取过程中发现职位均为JS动态加载的，导致信息抓取不完  整，故采用开启下载中间件，利用selenium + PhamtonJS的方式发送请求，  待数据加载完毕后返回提取。  自主爬取的网站：天气网，腾讯空间，职友集 |
| ---------- | ------------------------------------------------------------ |
|            |                                                              |





1.需求分析、技术方案选型，项目的架构设计、开发环境搭建；  

2.爬虫模块：对目标站点页面解析，进行深度抓取；

  3.中间件：构造请求和响应，处理特殊页面。

  4.管道模块：对关键目标信息进行持久化存储。

  5.爬取策略制定，友好抓取数据，防止反爬。  







腾讯招聘网  开发环境：Ubuntu、scrapy框架、scrapy-redis分布式组件、Redis  

项目简介：这个项目是对找工作方面信息的抓取，采用scrapy-redis分布式  实现。分布式使用Redis做为缓存数据库，利用Redis的高并发和I/O读写来  实现高速下载。抓取过程中发现职位均为JS动态加载的，导致信息抓取不完  整，故采用开启下载中间件，利用selenium + PhamtonJS的方式发送请求，  待数据加载完毕后返回提取。

  自主爬取的网站：天气网，腾讯空间，职友集  





## 2015.08 - 2015.11  中报远国家标准文献共享平台数据采集

| 责任描述： | (1)采用scrapy-redis分布式实现。  (2)攻破403反爬技术，所以必须通过Fiddler抓取二次访问的请求报头  (3)Request请求和指纹集合，并且对各个Slave端爬虫实现集中管理和控制  (4)采用MongoDB做为本地数据库，将采集到的数据保存到MongoDB中，同时每次下载前会检查请求指纹，防止重复下载，避免浪费资源 |
| ---------- | ------------------------------------------------------------ |
| 项目描述： | 该爬虫用于抓取NSSI国家标准文献库的标准号-标准名称-标准发布时间及执行时间的分类检索并入库 |

 sql的

增  insert

删 truncket  delete  ？？？？

查 select

改 uodate modify  ？？？？

语句

ajax加载出来的数据都是json吗，都是需要在控制台获取吗



## 2017.06 - 2017.06  爬取今日头条信息

| 软件环境： | ubuntu                                                       |
| ---------- | ------------------------------------------------------------ |
| 开发工具： | pycharm                                                      |
| 责任描述： | 独立爬取                                                     |
| 项目描述： | 1.爬取今日头条街拍内容，在第一级页面抓取标题和内容链接，在顺着链接抓取内容中的图片，并下载到Mongodb中  2.今日头条的信息是使用ajax动态加载的，所以和我之前爬取的猫眼并不 一样主要是在分析ajax加载的数据花了很长时间。  3.爬取数据的时候是使用正则来进行匹配的，所以在匹配数据的时候 花了很大的功夫。  4.在进行数据清洗的时候遇到了问题，因为之前对数据清洗这一块并没有 接触太多，提取到的数据并不是标准的json格式的数据，之前对  json.loads() 和json.dumps()作用模糊不清，最后查了很多资料才搞定，之后的问题就迎刃而解了 |

## 2017.06 - 2017.06  爬取今日头条信息

| 软件环境： | ubuntu                                                       |
| ---------- | ------------------------------------------------------------ |
| 开发工具： | pycharm                                                      |
| 责任描述： | 独立爬取                                                     |
| 项目描述： | 1.爬取今日头条街拍内容，在第一级页面抓取标题和内容链接，在顺着链接抓取内容中的图片，并下载到Mongodb中  2.今日头条的信息是使用ajax动态加载的，所以和我之前爬取的猫眼并不 一样主要是在分析ajax加载的数据花了很长时间。  3.爬取数据的时候是使用正则来进行匹配的，所以在匹配数据的时候 花了很大的功夫。  4.在进行数据清洗的时候遇到了问题，因为之前对数据清洗这一块并没有 接触太多，提取到的数据并不是标准的json格式的数据，之前对  json.loads() 和json.dumps()作用模糊不清，最后查了很多资料才搞定，之后的问题就迎刃而解了 |



## 2017.06 - 2017.08  票务类爬虫（猫眼、格瓦拉、时光网、微票）

| 责任描述： | 1．编写爬虫程序，分析站点。  2．解决反爬模拟用户请求。  3．保证爬虫数据的完整性。  4．编写票务类爬虫的说明文档  5．项目完毕，部署服务器。  6．每天定时抓取，保持数据更新！ |
| ---------- | ------------------------------------------------------------ |
| 项目描述： | 票务类爬虫项目，业务：电影售票门户网站的场次数据抓取和实时票务数据抓取。此项目主要抓取四个网站（猫眼、格瓦拉、时光网、微票）每天凌晨定时清除数据库表，然后抓取最新票务信息，存入数据库。 |



## 2017.02 - 2017.05  豆瓣网数据抓取系统

| 责任描述： | 1.分析网页的URL。  2.豆瓣网爬虫项目编写。  3.PhantomJS源码获取。  4.项目调试运行。 |
| ---------- | ------------------------------------------------------------ |
| 项目描述： | 该项目主要是获取评论，评论的内容，时间等，获取用户对电影的评价，在这个项目中使用MongoDB数据库，使用第三方库如：Xpath，Css等进行对数据的抽取。 |



## 017.01 - 至今  铁路智能货检

| 责任描述： | 1、参与需求分析及实现方案设计；  2、使用用HTML、CSS、Javascript、Jquery搭建问题信息、过车信息等前端页面；  3、前后端Ajax进行异步传输信息；  4、使用Django Web框架进行后台服务程序的编写  5、Linux服务器端部署项目； |
| ---------- | ------------------------------------------------------------ |
| 项目描述： | 该项目为铁路货运安检检测，运用深度深度学习。现场拍摄图片传至Web  Service，然后使用Soap解析XML并把图片信息保存至数据库，对货车图片进行处理辨别，判断车帮是否有异物，针对有问题车辆信息保存至数据库。货检人员根据时间、车型问题筛选数据，查看图片和识别结果作对比。 |

## 2015.10 - 2016.01  途猫物流

| 项目描述： | 该项目利用python语言Django开发。该项目分为司机端和货主端。途猫物流专为中小企业货主、批发商以及货代企业量身打造的带有支付功能的物流综合平台，最大限度地缩短物  流中间环节，促成交易实现，进而大幅度提升中国物流运行效率    责任描述：主要负责基本功能底层代码模块的实现和整个管理系统的开发。模块分为：司机货主车源货源信息的发布，会员充值，常跑路线，评论，添加广告，关注，反馈，搜索等 |
| ---------- | ------------------------------------------------------------ |
|            |                                                              |

## 2016.10 - 2017.04  二手车

| 项目描述： | 二手车买卖交易平台，O2O，接口设计符合RestfulAPI风格。  flask+mysql+redis,nginx |
| ---------- | ------------------------------------------------------------ |
|            |                                                              |

## 2016.07 - 2017.01  精灵系统

| 软件环境： | windows10.mysql ,django                                      |
| ---------- | ------------------------------------------------------------ |
| 硬件环境： | 8g运行内存                                                   |
| 开发工具： | pycharm,git                                                  |
| 责任描述： | 负责阳光保险公司的和核心业务的对接。编写平台代码，模仿阳光保险公司的 pc页面，http 请求然后 抓取页面获取页面上返回的必要值，再放入到请求中的参 数中请求核心系统，获得结果，实现快速投保。具体负责报价，通过车型险种，投  保人，等信息去请求核心获得报价信息，其中要根据业务逻辑去判断，返回给掌中 宝信息，比如，传过来报文中起保时间和终止时间的一个时间间隔，不同车型在使  年限上面的商业险折扣，等逻辑进行判断和计算，然后把这些信息传递给核心系统 进行报价计算。负责协助掌中宝的错误查询。 |
| 项目描述： | 仿照阳光保险公司的pc 端，开发出一套移动端的快速投保系统。相当于是把pc端逻辑还原  1、开发工具pycharm,django框架  2、使用fiddler 抓包工具进行抓包  3、Requests 模拟登陆，保存cookie登陆获取页面源码  4、使用Selenium PhantomJS ,模拟页面点击，调用js自动填充表单获得页面元素，  5、使用xpath 来解析html页面，选取元素  6、使用git 进行代码提交，版本控制等协同开发 |

## 017.02 - 至今  Python汽车租赁网站 Django Web开发

| 软件环境： | Python + Django + MySQL + Redis                              |
| ---------- | ------------------------------------------------------------ |
| 责任描述： | 1.对用户注册 , 登录及密码等信息进行增 , 删  , 改 , 查处理 . 注册时对用户名等信息进行唯一性校验 , 并在注册时通过邮箱进行二次验证 , 对用注册时的密码sha1加密后入库的操作 .   2.通过爬取分析其它汽车类型网站的热门车型和高评价车型 , 结合本站信息对本站车辆信息的维护和展示 . |
| 项目描述： | 公司网站开发, 这是一家专业的汽车租赁 , 汽车快修保养 , 车友会为一体的综合型企业 . 公司以专注“汽车售后服务”为宗旨 , 为广大客户提供超值的服务体验 . 本项目为用户提供线上选车约车等服务以及用户的交流 . |







信号管理是用dispatch模块