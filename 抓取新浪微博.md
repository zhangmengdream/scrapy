```
git@github.com:zhangmengdream/scrapy.git

#mainsrp-pager > div > div > div > ul > li:nth-child(4) > a
#mainsrp-pager > div > div > div > ul > li:nth-child(6) > a
```

```python
scrapy提供的一些比较好用的方法：（调试工具，做一些调试--命令行调试）
scrapy parse 方法--- 通过传入 url  和传入 url 解析函数，做到url的解析
# 这里的-c 相当于callback
#这里相当于请求这个url 用parse_detail进行解析  --  就可以运行出请求的结果了
scrapy parse （url）  -c   parse_detail  

```

cookie池  ip池的实现





第二种HTTPConnecitonPool

[Errno 11004] getaddrinfo failed
1
这种问题其实就是在上述问题的一种增强版，也就是说刚才你改好了代码，发现可以运行了，但是运行着运行着又断了，这种时候是说明你的速度达到了Pool的limit，应该把每次请求关闭，然后下一次请求就不会去占用Pool里面的资源了，解决方法便是将你要返回的内容赋值给一个变量然后关闭请求再返回那个变量，代码如下。

response.encoding = 'utf-8'
        if response.status_code == 200:
            time.sleep(1)
            content = response.text
            response.close()
            return content
        return None



###抓取猫眼电影并存储进txt

```python
# 多进程方式请求

import requests
#出现任何异常都会被request_exception捕捉到，其他异常都是这个类的子类
from requests.exceptions import RequestException
from multiprocessing import Pool
import re
import time
import json
headers = {
    # "Accept":"text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8",
    # "Accept-Encoding":"gzip, deflate",
    # "Accept-Language":"zh-CN,zh;q=0.8",
    # "Cache-Control":"max-age=0",
    # "Connection":"keep-alive",
    # "Cookie":"uuid=1A6E888B4A4B29B16FBA1299108DBE9C88D6E360CEA8509A4548A72D6631E7BC; _csrf=7d2e149f0778773d7eb8a62903ff87a4b6cf8746d45b39b5e5ab1391bfab611e; _lxsdk_cuid=16472e1dd15c8-078cd586855e46-1c29160a-dbc00-16472e1dd15c8; _lxsdk=1A6E888B4A4B29B16FBA1299108DBE9C88D6E360CEA8509A4548A72D6631E7BC; __mta=151911942.1530935762466.1530935762466.1530935762466.1; _lxsdk_s=16472e1dd17-eac-827-0c3%7C%7C3",
    # "Host":"maoyan.com",
    # "Upgrade-Insecure-Requests":"1",
    # "User-Agent":"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/61.0.3163.91 Safari/537.36"
    "Accept":"text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8",
    "Accept-Encoding":"gzip, deflate",
    "Accept-Language":"zh-CN,zh;q=0.9",
    "Cache-Control":"no-cache",
    "Connection":"keep-alive",
    "Cookie":"uuid=1A6E888B4A4B29B16FBA1299108DBE9C38E64EC48845ECD728C24F4D51F73771; _csrf=636e9101fa0b46044890ebe6e1bfc38e8eba33c5e8a69cbf152231e36a550418; _lxsdk_cuid=16472d0a744c8-04e42009b5c07c-47e1039-144000-16472d0a745c8; _lxsdk=1A6E888B4A4B29B16FBA1299108DBE9C38E64EC48845ECD728C24F4D51F73771; __mta=210064682.1530934634843.1530937106601.1530937108221.9; _lxsdk_s=16472f6603a-f11-53-dd8%7C%7C4",
    "Host":"maoyan.com",
    "Pragma":"no-cache",
    "Referer":"http://maoyan.com/board/4?offset=90",
    "Upgrade-Insecure-Requests":"1",
    "User-Agent":"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/67.0.3396.99 Safari/537.36",

}

def get_one_page(url):
    try:
        # requests.adapters.DEFAULT_RETRIES = 5
        response = requests.get(url,headers=headers,timeout=30)
        s = requests.session()
        s.keep_alive = False

        if response.status_code == 200:
            time.sleep(1)
            content = response.text
            response.close()
            return content
        return None
    except Exception as e:
        print(e)
        return None



# 解析代码
def parse_one_page(html):
    # 通过一个正则表达式的字符串，编译生成正则表达式的对象
    # re.S表示匹配任意字符，不加这个表示匹配一些换行符　　如果不加　.  匹配不了换行符，加了　. 就可以匹配到换行符
    pattern = re.compile('<dd>.*?board-index.*?>(\d+)</i>.*?data-src="(.*?)".*?name"><a'
                         +'.*?>(.*?)</a>.*?star">(.*?)</p>.*?releasetime">(.*?)</p>'
                         +'.*?integer">(.*?)</i>.*?fraction">(.*?)</i>.*?</dd>',re.S)
    items = re.findall(pattern,html)
    for item in items:
        yield {
         'index':item[0],
         'image':item[1],
         'title':item[2],
         'actor':item[3].strip()[3:],
         'time':item[4].strip()[5:],
         'score':item[5]+item[6]

        }
    # print(items)
def write_to_file(content):
    # 　如果写入的中文汉字变成unicode编码　就打开的时候　encoding='utf-8'　dumps的时候ensure_ascii=False
    #　这样就可以正常的输出了
    with open('result.txt','a',encoding='utf-8') as f:
        f.write(json.dumps(content,ensure_ascii=False)+'\n')
        f.close()

def main(offset):
    url = 'http://maoyan.com/board/4?offset='+str(offset)
    html = get_one_page(url)
    for item in parse_one_page(html):
        print(item)
        write_to_file(item)



if __name__ == '__main__':
    # for i in range(10):
    #     i = i*10
    #     main(i)
    #这个进程池可以提供指定数量的进程，然后用用户调用，如果有新的请求调用进程池，
    # 池还没满的话，创建新的请求调用进程，池如果已经满了的话就先等待
    pool = Pool()
    # 这个map 和 map的用法是一样的
    # 会将数组中的每一个元素拿出来，当做函数的参数,创建一个个的进程，放进进程池里面去运行
    pool.map(main,[i*10 for i in range(10)])
```



### 抓取今日头条街拍美图

分析：

1. 通过ajax构造一个ajax请求，把ajax请求的参数传递过来，调到网页的html代码，返回结果即可

2. 抓取网站详情页的内容

   通过ajax返回的json解析出来的文章列表，再进一步请求url就可以了 

3. 分析 变量，解析出图片

4. 把图片下载下来，图片的相关信息，名称，原始url等下载下来，保存进mongodb

5. 把所有的索引页，通过循环，改变offset抓取下来

```python
json还是字符串的格式，需要对字符串进行解析，响应的提取出url出来
用json.loads() 方法，把字符串解析成json对象


```

